{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee9fdadd-fa69-457e-bd68-f180c19c77b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+\n",
      "| _1|       _2|                  _3|\n",
      "+---+---------+--------------------+\n",
      "|  1|    Alice|   [HR, Recruitment]|\n",
      "|  2|      Bob|[Engineering, Sof...|\n",
      "|  3|Catherine|      [HR, Training]|\n",
      "+---+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField,StructType,IntegerType,StringType\n",
    "from pyspark.sql.functions import avg,count,max\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ComplexDataFrameExample\").getOrCreate()\n",
    "data = [\n",
    "    (1, 'Alice', ['HR', 'Recruitment']),\n",
    "    (2, 'Bob', ['Engineering', 'Software']),\n",
    "    (3, 'Catherine', ['HR', 'Training'])\n",
    "]\n",
    "df_complex = spark.createDataFrame(data)\n",
    "df_complex.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9af17228-b11b-4942-a624-1d962f7a292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+------+\n",
      "| id| name| department|salary|\n",
      "+---+-----+-----------+------+\n",
      "|  1|Alice|         HR| 70000|\n",
      "|  2|  Bob|Engineering| 80000|\n",
      "+---+-----+-----------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField,StructType,IntegerType,StringType\n",
    "\n",
    "data = [\n",
    "    (1, 'Alice', 'HR', 70000),\n",
    "    (2, 'Bob', 'Engineering', 80000),\n",
    "    (3, 'Catherine', 'HR', 75000),\n",
    "    (4, 'David', 'Engineering', 95000),\n",
    "    (5, 'Eva', 'Marketing', 60000)\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "df_salaries = spark.createDataFrame(data, schema)\n",
    "df_salaries.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e03ff8-43aa-4b17-8202-95d6d53d8090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+----------+\n",
      "| department|average_salary|employee_count|max_salary|\n",
      "+-----------+--------------+--------------+----------+\n",
      "|         HR|       72500.0|             2|     75000|\n",
      "|Engineering|       87500.0|             2|     95000|\n",
      "|  Marketing|       60000.0|             1|     60000|\n",
      "+-----------+--------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_salaries.groupBy(\"department\").agg(\n",
    "    avg(\"salary\").alias(\"average_salary\"),\n",
    "    count(\"id\").alias(\"employee_count\"),\n",
    "    max(\"salary\").alias(\"max_salary\")\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47e1f1c6-eae3-46f1-88d5-5070080de73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+\n",
      "| id|     name|salary|\n",
      "+---+---------+------+\n",
      "|  1|    Alice| 70000|\n",
      "|  2|  Unknown| 80000|\n",
      "|  3|Catherine|     0|\n",
      "|  4|    David| 95000|\n",
      "+---+---------+------+\n",
      "\n",
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|Alice| 70000|\n",
      "|  4|David| 95000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 'Alice', 70000),\n",
    "    (2, None, 80000),\n",
    "    (3, 'Catherine', None),\n",
    "    (4, 'David', 95000)\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "df_missing = spark.createDataFrame(data, schema)\n",
    "df_filled = df_missing.fillna({'name': 'Unknown', 'salary': 0})\n",
    "df_dropped = df_missing.dropna()\n",
    "df_filled.show()\n",
    "df_dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38eb5010-2ed8-4b44-b9eb-64497d89f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, name='Alice'), Row(id=2, name='Bob')]\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "result = df.collect()\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2932d765-17c4-4dee-a577-f8650cd6b64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "print(df.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f0e7b95-a6ee-4070-a568-266a28d44000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=1, name='Alice')\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "print(df.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dac4cad5-e4fc-481c-afab-d7c906d01d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, name='Alice'), Row(id=2, name='Bob')]\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dfaf677-721a-4b14-9488-0d5eb661176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, name='Alice')]\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "print(df.take(1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "887ea5c6-285d-4393-8f08-64ef41ccc13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "+---+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "print(df.show(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5205ccab-0a45-4982-aed7-99a4403ca172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, id: string, salary: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 100), (2, 200)], [\"id\", \"salary\"])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1755601b-a59f-4584-ac2a-29309b3149ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Alice|   100|\n",
      "|  Bob|   200|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\", 100), (2, \"Bob\", 200)], [\"id\", \"name\", \"salary\"])\n",
    "df.select(\"name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26de109a-c4c4-406f-a371-ed39ebae47d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|total_salary|average_salary|\n",
      "+------------+--------------+\n",
      "|         300|         150.0|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg\n",
    "df = spark.createDataFrame([(1, 100), (2, 200)], [\"id\", \"salary\"])\n",
    "df.agg(sum(\"salary\").alias(\"total_salary\"), avg(\"salary\").alias(\"average_salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cc998bd-e829-48cd-a1ec-c053bb6e8814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.rdd.foreach(lambda row : print(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7c25d90-9bd5-4298-b042-4da2418d2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.foreachPartition(lambda partition: print(list(partition)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e40672dc-da5a-437f-8c62-266c29280a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   name\n",
      "0   1  Alice\n",
      "1   2    Bob\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "pandas_df = df.toPandas()\n",
    "print(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ee87f47-c3a3-48a7-82fb-6969da42ba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d74cf373-9a15-49dd-99c0-b11cad53e9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  3|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Alice\"), (3, \"Bob\")], [\"id\", \"name\"])\n",
    "df.dropDuplicates([\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c67e9-0a5c-4378-aa53-e4729bb65ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
